{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiragm/DeepLearnng/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece"
      ],
      "metadata": {
        "id": "7jTW9EuDmn2w",
        "outputId": "be75422a-2769-490b-85c4-0013a9c154dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "BaSWnw7UE1rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 1. テキストデータを用意します（例えば、'sample_text.txt'という名前のファイルに保存）。\n",
        "# このデータは、SentencePieceがサブワードモデルを学習するためのものです。\n",
        "\"\"\"\n",
        "sample_text.txtの内容:\n",
        "これはテストテキストです。\n",
        "SentencePieceは効果的なトークン化ツールです。\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "# 2. SentencePieceモデルの学習\n",
        "spm.SentencePieceTrainer.train('--input=/content/AI_traning_data.txt --model_prefix=m --vocab_size=2000')\n",
        "\n",
        "# 3. 学習したモデルをロード\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# 4. テキストをトークン化\n",
        "text = \"Y4@DB4G56GA.00\"\n",
        "print(sp.encode_as_pieces(text))"
      ],
      "metadata": {
        "id": "pbABD2yx_ARf",
        "outputId": "4b4db4c8-45f7-4496-fa38-08294d33610a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', 'Y', '4@', 'DB', '4', 'G', '56', 'GA', '.0', '0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# トークンIDへの変換\n",
        "tokenized_text = sp.encode_as_ids(text)\n",
        "\n",
        "# Transformerモデルの設定\n",
        "d_model = 512  # 例として、モデルの次元を512とします\n",
        "vocab_size = 2000  # 上で設定したvocab_size\n",
        "max_seq_length = 100  # 例として、最大シーケンス長を100とします\n",
        "\n",
        "# 3. Embedding\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "embedding_layer = EmbeddingLayer(vocab_size, d_model)\n",
        "\n",
        "# トークンIDをTensorに変換し、Embedding\n",
        "input_tensor = torch.tensor(tokenized_text).unsqueeze(0)  # バッチの次元を追加\n",
        "embedded = embedding_layer(input_tensor)\n",
        "\n",
        "# 4. Transformerの利用\n",
        "transformer_layer = nn.Transformer(d_model, nhead=8, num_encoder_layers=6)\n",
        "output = transformer_layer(embedded, embedded)\n",
        "\n",
        "# 5. タスク固有のヘッド (ここでは分類タスクを例としています)\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.linear = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ここで、[CLS]トークンとして最初のトークンを使用します\n",
        "        return self.linear(x[:, 0, :])\n",
        "\n",
        "num_classes = 10  # 例: 10クラス分類\n",
        "classification_head = ClassificationHead(d_model, num_classes)\n",
        "class_logits = classification_head(output)\n",
        "\n",
        "print(class_logits)  # 分類タスクのロジットを表示\n"
      ],
      "metadata": {
        "id": "udmn_dABBEFp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Colab の定期購入を最大限に活用する",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}